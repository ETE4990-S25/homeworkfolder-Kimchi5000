{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f26ddd3",
   "metadata": {},
   "source": [
    "\n",
    "# Lab 9: Build a Log Aggregator\n",
    "\n",
    "In this lab, you will create your own log generator, build a command-line utility that scans log files, summarizes their contents, and provides insight into system behavior. Data structures to track log message levels such as `INFO`, `WARNING`, `ERROR`, and `CRITICAL`.\n",
    "\n",
    "This lab reinforces:\n",
    "- File I/O\n",
    "- Pattern recognition (regex)\n",
    "- Dictionaries and counters\n",
    "- Functions and modularity\n",
    "- CLI arguments, logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d5ee8a",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Create Log files (20%)\n",
    "Using the the following example log format below create a **python file** that will log errors In a structured tree format \n",
    "\n",
    "You will find examples in the folder called Logs that you can use to build your program.\n",
    "\n",
    "Remember set of logs should have a varied levels of log entries (`INFO`, `WARNING`, `ERROR`, `CRITICAL`) and tailored message types for different service components.\n",
    "You must create 5 structured logs here are some examples:\n",
    "\n",
    "    sqldb\n",
    "    ui\n",
    "    frontend.js\n",
    "    backend.js\n",
    "    frontend.flask\n",
    "    backend.flask\n",
    "\n",
    "You may use chat GPT to create sample outputs NOT THE LOGS. IE:\n",
    "\n",
    "    System failure\n",
    "    Database corruption\n",
    "    Disk failure detected\n",
    "    Database corruption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ba30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Create logger for file logging\n",
    "logger = logging.getLogger(\"file_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create file handler\n",
    "file_handler = logging.FileHandler(\"example_log.log\")\n",
    "file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "\n",
    "# Add handler to the logger\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "loggers = {\n",
    "    \"sqldb\": logging.getLogger(\"sqldb\"),\n",
    "    \"ui\": logging.getLogger(\"ui\"),\n",
    "    \"frontend.js\": logging.getLogger(\"frontend.js\"),\n",
    "    \"backend.js\": logging.getLogger(\"backend.js\"),\n",
    "    \"frontend.flask\": logging.getLogger(\"frontend.flask\"),\n",
    "    \"backend.flask\": logging.getLogger(\"backend.flask\"),\n",
    "}\n",
    "\n",
    "for log in loggers.values():\n",
    "    log.addHandler(file_handler)\n",
    "    log.setLevel(logging.INFO) \n",
    "\n",
    "def log_messages():\n",
    "    loggers[\"sqldb\"].info(\"user logged in with correct password\")\n",
    "    loggers[\"sqldb\"].error(\"file not found\")\n",
    "    loggers[\"ui\"].warning(\"disk space low\")\n",
    "    loggers[\"frontend.js\"].critical(\"there is no more disk space\")\n",
    "    loggers[\"backend.flask\"].error(\"file not found\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    log_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5255ab",
   "metadata": {},
   "source": [
    "\n",
    "### Example Log Format\n",
    "\n",
    "You will work with logs that follow this simplified structure:\n",
    "\n",
    "```\n",
    "2025-04-11 23:20:36,913 | my_app | INFO | Request completed\n",
    "2025-04-11 23:20:36,914 | my_app.utils | ERROR | Unhandled exception\n",
    "2025-04-11 23:20:36,914 | my_app.utils.db | CRITICAL | Disk failure detected\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3659dfbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af5f6e84",
   "metadata": {},
   "source": [
    "## Part 2: Logging the Log File (40%)\n",
    "    New File\n",
    "### Part 2a: Read the Log File (see lab 7) (10%)\n",
    "\n",
    "\n",
    "Write a function to read the contents of a log file into a list of lines. Handle file errors gracefully.\n",
    "\n",
    "### Part 2b: Parse Log Lines (see code below if you get stuck) (10%)\n",
    "\n",
    "Use a regular expression to extract:\n",
    "- Timestamp\n",
    "- Log name\n",
    "- Log level\n",
    "- Message\n",
    "\n",
    "### Part 2c: Count Log Levels (20%)\n",
    "\n",
    "Create a function to count how many times each log level appears. Store the results in a dictionary. Then output it as a Json File\n",
    "You may pick your own format but here is an example. \n",
    "```python\n",
    "{\n",
    "    \"INFO\": \n",
    "    {\n",
    "        \"Request completed\": 42, \n",
    "        \"Heartbeat OK\": 7\n",
    "    }\n",
    "\n",
    "    \"WARNING\":\n",
    "    {\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc631f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2a\n",
    "def read_the_log_file(filename):\n",
    "    log_lines = [] \n",
    "    try:\n",
    "        with open(filename, 'r') as file: \n",
    "            log_lines = file.readlines()  # Read all lines into the list\n",
    "        print(\"file read successfully\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"file not found\")\n",
    "\n",
    "    return log_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8a0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2b\n",
    "import re\n",
    "\n",
    "def parse_log_lines(log_lines):\n",
    "    # Define the regex pattern to match log entries\n",
    "    regex_pattern = r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3}) - (\\w+) - (\\w+) - (.*)'\n",
    "\n",
    "    parsed_logs = [] \n",
    "    for line in log_lines:\n",
    "        match = re.match(regex_pattern, line.strip())  # Match the line against the regex pattern\n",
    "        if match:\n",
    "            log_entry = {\n",
    "                'timestamp': match.group(1),\n",
    "                'log_name': match.group(2),\n",
    "                'log_level': match.group(3),\n",
    "                'message': match.group(4)\n",
    "            }\n",
    "            parsed_logs.append(log_entry)  # Add the parsed entry to the list\n",
    "        else:\n",
    "            print(\"Log entry does not match the expected format:\", line)\n",
    "\n",
    "    return parsed_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae26fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2c\n",
    "import json\n",
    "\n",
    "def count_log_levels(parsed_logs):\n",
    "    log_level_count = {}\n",
    "\n",
    "    for log in parsed_logs:\n",
    "        level = log['log_level']\n",
    "        message = log['message']\n",
    "\n",
    "        if level not in log_level_count:\n",
    "            log_level_count[level] = {}\n",
    "\n",
    "        if message not in log_level_count[level]:\n",
    "            log_level_count[level][message] = 0\n",
    "\n",
    "        log_level_count[level][message] += 1\n",
    "\n",
    "    return log_level_count\n",
    "\n",
    "def save_to_json(log_counts, filename='log_counts.json'):\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(log_counts, json_file, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    log_file_name = 'example_log.log' \n",
    "    parsed_log_entries = read_the_log_file(log_file_name)  \n",
    "\n",
    "    log_counts = count_log_levels(parsed_log_entries)\n",
    "\n",
    "    save_to_json(log_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045c30f",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Generate Summary Report (40%)\n",
    "    New File\n",
    "### Step 3a (20%):\n",
    " Develop a function that continuously monitors your JSON file(s) and will print a real-time summary of log activity. It should keep count of the messages grouped by log level (INFO, WARNING, ERROR, CRITICAL) and display only the critical messages. (I.e. If new data comes in the summary will change and a new critical message will be printed)\n",
    " - note: do not reprocess the entire file on each update.  \n",
    "\n",
    "### Step 3b: Use a Matplotlib (Lecture 10) (20%)\n",
    "Develop a function that continuously monitors your JSON file(s) and will graph in real-time a bar or pie plot of each of the errors.  (a graph for each log level). \n",
    "- The graph should show the distribution of log messages by level  (INFO, WARNING, ERROR, CRITICAL)  \n",
    "\n",
    "\n",
    "### Critical notes:\n",
    "- Your code mus use Daemon Threads (Lecture 14)\n",
    "- 3a and 3b do not need to run at the same time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea4429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3a\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "\n",
    "LOG_FILE = \"log_counts.json\" \n",
    "WAIT_TIME = 1 #Time between checks\n",
    "\n",
    "# Function to load the JSON log summary\n",
    "def load_logs(filepath):\n",
    "    try:\n",
    "        with open(filepath, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(\"Problem reading the log summary:\", e)\n",
    "        return {}\n",
    "\n",
    "# Keeps track of new messages\n",
    "previous_criticals = {}\n",
    "\n",
    "# Main monitoring function\n",
    "def watch_logs():\n",
    "    last_updated = 0\n",
    "\n",
    "    while True:\n",
    "        if os.path.exists(LOG_FILE):\n",
    "            current_time = os.path.getmtime(LOG_FILE)\n",
    "            if current_time != last_updated:\n",
    "                last_updated = current_time\n",
    "                logs = load_logs(LOG_FILE)\n",
    "\n",
    "                print(\"\\nLog Summary\")\n",
    "                for level, msgs in logs.items():\n",
    "                    total = sum(msgs.values())\n",
    "                    print(f\"{level}: {total} entries\")\n",
    "\n",
    "                    if level == \"CRITICAL\":\n",
    "                        for msg, count in msgs.items():\n",
    "                        \n",
    "                            old_count = previous_criticals.get(msg, 0)\n",
    "                            if count > old_count:\n",
    "                                print(f\"NEW CRITICAL: {msg} ({count})\")\n",
    "                                previous_criticals[msg] = count\n",
    "\n",
    "        time.sleep(WAIT_TIME)\n",
    "\n",
    "# Start thread\n",
    "if __name__ == \"__main__\":\n",
    "    t = threading.Thread(target=watch_logs)\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "\n",
    "    print(\"Monitoring logs. Press Ctrl+C to stop\\n\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopped monitoring.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26eb058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3b\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LOG_FILE = \"log_summary.json\" \n",
    "WAIT_TIME = 1\n",
    "\n",
    "\n",
    "def load_logs():\n",
    "    try:\n",
    "        with open(LOG_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(\"Couldn't read log file:\", e)\n",
    "        return {}\n",
    "\n",
    "# Main function for plotting\n",
    "def plot_logs():\n",
    "    plt.ion()  \n",
    "    fig, ax = plt.subplots()\n",
    "    last_updated = 0\n",
    "\n",
    "    while True:\n",
    "        if os.path.exists(LOG_FILE):\n",
    "            current_time = os.path.getmtime(LOG_FILE)\n",
    "            if current_time != last_updated:\n",
    "                last_updated = current_time\n",
    "                logs = load_logs()\n",
    "\n",
    "                # Reset chart\n",
    "                ax.clear()\n",
    "\n",
    "                levels = []\n",
    "                totals = []\n",
    "\n",
    "                for level, msgs in logs.items():\n",
    "                    levels.append(level)\n",
    "                    totals.append(sum(msgs.values()))\n",
    "\n",
    "                ax.bar(levels, totals, color=['blue', 'orange', 'yellow', 'red'])\n",
    "                ax.set_title(\"Log Level Distribution\")\n",
    "                ax.set_ylabel(\"Count\")\n",
    "                ax.set_xlabel(\"Log Level\")\n",
    "\n",
    "                plt.draw()\n",
    "                plt.pause(0.1)\n",
    "\n",
    "        time.sleep(WAIT_TIME)\n",
    "\n",
    "# Start graphing in a daemon thread\n",
    "if __name__ == \"__main__\":\n",
    "    t = threading.Thread(target=plot_logs)\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "\n",
    "    print(\"Graphing logs. Press Ctrl+C to stop\\n\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopped graphing.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
